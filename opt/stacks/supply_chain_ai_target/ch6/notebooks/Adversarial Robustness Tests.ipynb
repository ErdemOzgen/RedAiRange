{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26d6a44b-9382-425a-9863-2fd186c20f9a",
   "metadata": {},
   "source": [
    "# Using Adversarial Robustness Tests to evaluate pre-trained models \n",
    "\n",
    "This notebook demonstratesthe use of ML-generated samplest to test adversarial robustness of the pretrained models to detect anomalies. We will be using a technique called FSGM and is used to generate changed samples using subtle changes called perturbations. We discuss this technique and perturbations in the next chapter. Here we want to detect signs of anomalies in the performance of pre-trained models with avdersarial data to investigate further.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee55c1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import art\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.DEBUG)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "# prevent memory error messages in GPU environments by setting memory growth equal to all GPUs \n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "559c63f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yanni/.local/lib/python3.10/site-packages/keras/src/layers/normalization/batch_normalization.py:883: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanni/.local/lib/python3.10/site-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2024-06-29 14:35:20.194643: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape indropout/cond/then/_56/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "/home/yanni/.local/lib/python3.10/site-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for ../models/simple-cifar10-cnn.h5\n",
      "Accuracy on clean test samples: 0.8665000200271606\n",
      "Accuracy on adversarial test samples: 0.11400000005960464\n",
      "Average confidence on clean test samples: 0.9276211857795715\n",
      "Average confidence on adversarial test samples: 0.7819012403488159\n",
      "\n",
      "\n",
      "Results for ../models/simple-cifar10-poisoned.h5\n",
      "Accuracy on clean test samples: 0.678600013256073\n",
      "Accuracy on adversarial test samples: 0.09650000184774399\n",
      "Average confidence on clean test samples: 0.845690906047821\n",
      "Average confidence on adversarial test samples: 0.8059046268463135\n",
      "\n",
      "\n",
      "Results for ../models/backdoor-pattern-cifar10.h5\n",
      "Accuracy on clean test samples: 0.10019999742507935\n",
      "Accuracy on adversarial test samples: 0.0997999981045723\n",
      "Average confidence on clean test samples: 0.4350295960903168\n",
      "Average confidence on adversarial test samples: 0.4363557994365692\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from art.attacks.evasion import FastGradientMethod\n",
    "from art.estimators.classification import KerasClassifier\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model_robustness(model_path , eps=0.1):\n",
    "    \"\"\"\n",
    "    Evaluate the robustness of a given model against adversarial attacks using FGSM\n",
    "    \"\"\"\n",
    "    # load the model \n",
    "    model = load_model(model_path)\n",
    "    # Load and preprocess CIFAR-10 data\n",
    "    (_, _), (x_test, y_test) = cifar10.load_data()\n",
    "    x_test = x_test.astype('float32') / 255\n",
    "\n",
    "    # Convert integer labels to one-hot encoded labels\n",
    "    y_test_one_hot = to_categorical(y_test)\n",
    "\n",
    "    # Wrap the model with ART's KerasClassifier\n",
    "    classifier = KerasClassifier(model=model, clip_values=(0, 1))\n",
    "\n",
    "    # Generate adversarial examples using FGSM\n",
    "    attack = FastGradientMethod(estimator=classifier, eps=eps)\n",
    "    x_test_adv = attack.generate(x=x_test)\n",
    "\n",
    "    # Evaluate the model on clean and adversarial samples\n",
    "    _, clean_accuracy = model.evaluate(x_test, y_test_one_hot, verbose=0)\n",
    "    _, adv_accuracy = model.evaluate(x_test_adv, y_test_one_hot, verbose=0)\n",
    "    print(f\"Results for {model_path}\")\n",
    "    # Print evaluation results\n",
    "    print(f\"Accuracy on clean test samples: {clean_accuracy}\")\n",
    "    print(f\"Accuracy on adversarial test samples: {adv_accuracy}\")\n",
    "\n",
    "    # Analyze confidence scores on clean and adversarial samples\n",
    "    clean_predictions = classifier.predict(x_test)\n",
    "    adv_predictions = classifier.predict(x_test_adv)\n",
    "\n",
    "    clean_confidence = np.max(clean_predictions, axis=1).mean()\n",
    "    adv_confidence = np.max(adv_predictions, axis=1).mean()\n",
    "\n",
    "    # Print confidence analysis\n",
    "    print(f\"Average confidence on clean test samples: {clean_confidence}\")\n",
    "    print(f\"Average confidence on adversarial test samples: {adv_confidence}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Load pre-trained model\n",
    "evaluate_model_robustness('../models/simple-cifar10-cnn.h5')\n",
    "evaluate_model_robustness('../models/simple-cifar10-poisoned.h5')\n",
    "evaluate_model_robustness('../models/backdoor-pattern-cifar10.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
